üìò SmartCamRT ‚Äì Milestone Learning Checklist

This checklist guides you through building SmartCamRT from scratch. Each milestone builds on the last, helping you learn AI runtime environments, model compression, and edge deployment.

‚∏ª

üß† Context for LLMs / Assistants

üìå Project Description

I‚Äôm building a real-time AI inference engine called SmartCamRT that runs optimized DNN models (e.g., YOLOv5, LSTM, BERT) on edge devices using different runtimes like ONNX Runtime, TensorRT, TFLite, and ExecuTorch. It supports runtime switching, model compression, performance logging, and deployment to Raspberry Pi/Android. I‚Äôm following a milestone-based plan to gradually build and optimize each component.

üóÇ Repo Layout

My project has a core/ folder for production-ready modules and a learn_and_replicate/ folder for my step-by-step development process. Each milestone builds one part of the final system (e.g., runtime wrapper, logger, optimizer, edge deployment).

üßæ Prompt Template

I‚Äôm following a milestone-based plan stored in milestone_checklist.md. Here‚Äôs the checklist: [paste it]. I‚Äôm currently working on milestone X. Can you help me with [describe your task]?

‚∏ª

‚úÖ Milestone 1: Run DNN Model with ONNX Runtime
	‚Ä¢	Convert YOLOv5 from PyTorch to ONNX.
	‚Ä¢	Load and run ONNX model on webcam frames using ONNX Runtime (Python).
	‚Ä¢	Measure inference latency per frame.
	‚Ä¢	Save results/logs to CSV or file.

‚∏ª

‚úÖ Milestone 2: Add TensorRT and TensorFlow Lite Support
	‚Ä¢	Convert ONNX to TensorRT engine using trtexec or API.
	‚Ä¢	Convert model to TFLite using TensorFlow converter.
	‚Ä¢	Implement runtime switch between ONNX, TensorRT, and TFLite in Python.
	‚Ä¢	Benchmark all runtimes on the same input.

‚∏ª

‚úÖ Milestone 3: Build Performance Logger
	‚Ä¢	Log CPU/memory usage using psutil.
	‚Ä¢	Log GPU usage using py3nvml or NVIDIA tools.
	‚Ä¢	Track per-frame latency, CPU, and memory usage.
	‚Ä¢	Plot performance trends with matplotlib or show CLI summary.

‚∏ª

‚úÖ Milestone 4: Real-Time Optimizer Logic
	‚Ä¢	Monitor system load (CPU/GPU).
	‚Ä¢	Automatically switch to lighter models/runtimes under load.
	‚Ä¢	Integrate fallback to YOLOv5n or similar.
	‚Ä¢	Stress test with simultaneous inferences.

‚∏ª

‚úÖ Milestone 5: C++ Inference Path
	‚Ä¢	Load and run model with ONNX Runtime or TensorRT C++ API.
	‚Ä¢	Recreate inference loop and logger in C++.
	‚Ä¢	Measure and compare C++ vs Python performance.
	‚Ä¢	Optional: Add multithreading for throughput.

‚∏ª

‚úÖ Milestone 6: LSTM and Transformer Models
	‚Ä¢	Run inference using an LSTM-based model (e.g., sentiment analysis).
	‚Ä¢	Run a transformer model (e.g., DistilBERT or Whisper).
	‚Ä¢	Compare performance of CNN, LSTM, and Transformer models.
	‚Ä¢	Log latency, memory, and differences in runtime behavior.

‚∏ª

‚úÖ Milestone 7: Integrate ExecuTorch
	‚Ä¢	Export a lightweight model with torch.export for ExecuTorch.
	‚Ä¢	Deploy or simulate on an ExecuTorch-compatible environment (e.g., Android).
	‚Ä¢	Run inference and measure performance.
	‚Ä¢	Compare ExecuTorch with other runtimes.

‚∏ª

‚úÖ Milestone 8: Model Compression for Edge
	‚Ä¢	Apply post-training quantization (ONNX/PyTorch/TFLite).
	‚Ä¢	Experiment with quantization-aware training if accuracy drops.
	‚Ä¢	Apply pruning using torch.nn.utils.prune.
	‚Ä¢	Compare size, speed, and accuracy before and after compression.
	‚Ä¢	Optional: Perform knowledge distillation from a large to small model.

‚∏ª

‚úÖ Milestone 9: Deploy to Edge Device
	‚Ä¢	Deploy ONNX/TFLite model to Raspberry Pi (Option A).
	‚Ä¢	Deploy PyTorch/ExecuTorch model to Android (Option B).
	‚Ä¢	Measure on-device latency and resource usage.
	‚Ä¢	Document deployment constraints and performance.

‚∏ª

üèÅ Bonus: Final Capstone Summary
	‚Ä¢	Write a report summarizing:
	‚Ä¢	Runtime and model comparisons
	‚Ä¢	Optimization and compression results
	‚Ä¢	Edge deployment insights
	‚Ä¢	Visuals: charts, tables, logs
	‚Ä¢	Move production-ready code to smartcamrt/core/
	‚Ä¢	Clean up experiment code in learn_and_replicate/
	‚Ä¢	Refer to [`capstone_summary_template.md`](./capstone_summary_template.md) for guidance

‚∏ª

You now have a portfolio-ready, production-quality edge AI optimizer.

Happy building! üöÄ